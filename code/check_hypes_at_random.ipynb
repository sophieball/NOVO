{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db83a8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "import logging\n",
    "import os\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer as CV\n",
    "\n",
    "from langdetect import detect\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88ba01c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-0510137ad2ba>:14: DtypeWarning: Columns (21,41,45,52) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dat = pd.read_csv(\"../data/reg_data_R.csv\", dtype={'Application_id': str,})\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet(\"../data/20220602 - Application full text.parquet\",\n",
    "                     engine=\"fastparquet\",\n",
    "#                      header=0, \n",
    "#                      dtype={'Application_id': str,})\n",
    "#                      \"clean_text\": str}\n",
    "                    )\n",
    "df = df[[\"Application_id\", \"Project_description_clean\"]]\n",
    "df = df.dropna()\n",
    "# df = df.fillna(0)\n",
    "# df[\"text\"] = df[\"clean_text\"]\n",
    "df[\"text\"] = df[\"Project_description_clean\"]\n",
    "# custom_stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "dat = pd.read_csv(\"../data/reg_data_R.csv\", dtype={'Application_id': str,})\n",
    "dat_df = df.merge(dat, on=\"Application_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16ea186e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, tokenizer):\n",
    "    \"\"\"Pre-process text and generate tokens\n",
    "\n",
    "    Args:\n",
    "        text: Text to tokenize.\n",
    "\n",
    "    Returns:\n",
    "        Tokenized text.\n",
    "    \"\"\"\n",
    "    text = str(text).lower()  # Lowercase words\n",
    "#     text = re.sub(r\"\\[(.*?)\\]\", \"\", text)  # Remove [+XYZ chars] in content\n",
    "#     text = re.sub(r\"\\s+\", \" \", text)  # Remove multiple spaces in content\n",
    "#     text = re.sub(r\"\\w+|\", \"\", text)  # Remove ellipsis (and last word)\n",
    "#     text = re.sub(r\"(?<=\\w)-(?=\\w)\", \" \", text)  # Replace dash between words\n",
    "    text = re.sub(\n",
    "        f\"[{re.escape(string.punctuation)}]\", \"\", text\n",
    "    )  # Remove punctuation\n",
    "    \n",
    "#     text = re.sub(\"\\d+\", \"\", text) # remove digits\n",
    "\n",
    "    tokens = tokenizer(text)  # Get tokens from text\n",
    "#     tokens = [t for t in tokens if not t in stopwords]  # Remove stopwords\n",
    "#     tokens = [\"\" if t.isdigit() else t for t in tokens]  # Remove digits\n",
    "    tokens = [t for t in tokens if len(t) > 1]  # Remove short tokens\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1193dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_df[\"text\"] = dat_df[\"text\"].fillna(\"\")\n",
    "\n",
    "# dat_df[\"english\"] = dat_df[\"text\"].map(lambda x:detect(x)==\"en\")\n",
    "\n",
    "dat_df[\"tokens\"] = dat_df[\"text\"].map(lambda x: clean_text(x, word_tokenize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "961edaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "hype_dict = {\n",
    "'Importance': ['compelling',\n",
    " 'critical',\n",
    " 'crucial',\n",
    " 'essential',\n",
    " 'foundational',\n",
    " 'fundamental',\n",
    " 'imperative',\n",
    " 'important',\n",
    " 'indispensable',\n",
    " 'invaluable',\n",
    " 'key',\n",
    " 'major',\n",
    " 'paramount',\n",
    " 'pivotal',\n",
    " 'significant',\n",
    " 'strategic',\n",
    " 'timely',\n",
    " 'ultimate',\n",
    " 'urgent',\n",
    " 'vital'],\n",
    "\n",
    "'Novelty': ['creative',\n",
    " 'emerging',\n",
    " 'first',\n",
    " 'groundbreaking',\n",
    " 'innovative',\n",
    " 'latest',\n",
    " 'novel',\n",
    " 'revolutionary',\n",
    " 'unique',\n",
    " 'unparalleled',\n",
    " 'unprecedented'],\n",
    "             \n",
    "'Rigor': ['accurate',\n",
    " 'advanced',\n",
    " 'careful',\n",
    " 'cohesive',\n",
    " 'detailed',\n",
    " 'nuanced',\n",
    " 'powerful',\n",
    " 'quality',\n",
    " 'reproducible',\n",
    " 'rigorous',\n",
    " 'robust',\n",
    " 'scientific',\n",
    " 'sophisticated',\n",
    " 'strong',\n",
    " 'systematic'],\n",
    "             \n",
    "'Scale': ['ample',\n",
    " 'biggest',\n",
    " 'broad',\n",
    " 'comprehensive',\n",
    " 'considerable',\n",
    " 'deeper',\n",
    " 'diverse',\n",
    " 'enormous',\n",
    " 'expansive',\n",
    " 'extensive',\n",
    " 'fastest',\n",
    " 'greatest',\n",
    " 'huge',\n",
    " 'immediate',\n",
    " 'immense',\n",
    " 'interdisciplinary',\n",
    " 'international',\n",
    " 'interprofessional',\n",
    " 'largest',\n",
    " 'massive',\n",
    " 'multidisciplinary',\n",
    " 'myriad',\n",
    " 'overwhelming',\n",
    " 'substantial',\n",
    " 'top',\n",
    " 'transdisciplinary',\n",
    " 'tremendous',\n",
    " 'vast'],\n",
    "             \n",
    "'Utility': ['accessible',\n",
    " 'actionable',\n",
    " 'deployable',\n",
    " 'durable',\n",
    " 'easy',\n",
    " 'effective',\n",
    " 'efficacious',\n",
    " 'efficient',\n",
    " 'generalizable',\n",
    " 'ideal',\n",
    " 'impactful',\n",
    " 'intuitive',\n",
    " 'meaningful',\n",
    " 'productive',\n",
    " 'ready',\n",
    " 'relevant',\n",
    " 'rich',\n",
    " 'safer',\n",
    " 'scalable',\n",
    " 'seamless',\n",
    " 'sustainable',\n",
    " 'synergistic',\n",
    " 'tailored',\n",
    " 'tangible',\n",
    " 'transformative',\n",
    " 'userfriendly'],\n",
    "             \n",
    "'Quality': ['ambitious',\n",
    " 'collegial',\n",
    " 'dedicated',\n",
    " 'exceptional',\n",
    " 'experienced',\n",
    " 'intellectual',\n",
    " 'longstanding',\n",
    " 'motivated',\n",
    " 'premier',\n",
    " 'prestigious',\n",
    " 'promising',\n",
    " 'qualified',\n",
    " 'renowned',\n",
    " 'senior',\n",
    " 'skilled',\n",
    " 'stellar',\n",
    " 'successful',\n",
    " 'talented',\n",
    " 'vibrant'],\n",
    "             \n",
    "'Attitude': ['attractive',\n",
    " 'confident',\n",
    " 'exciting',\n",
    " 'incredible',\n",
    " 'interesting',\n",
    " 'intriguing',\n",
    " 'notable',\n",
    " 'outstanding',\n",
    " 'remarkable',\n",
    " 'surprising'],\n",
    "             \n",
    "'Problem': ['alarming',\n",
    " 'daunting',\n",
    " 'desperate',\n",
    " 'devastating',\n",
    " 'dire',\n",
    " 'dismal',\n",
    " 'elusive',\n",
    " 'stark',\n",
    " 'unanswered',\n",
    " 'unmet']\n",
    "}\n",
    "\n",
    "hype_list = list(set([j for l in hype_dict.values() for j in l]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5960a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get hype words' synonyms\n",
    "hype_syns = []\n",
    "\n",
    "hype_and_syns = set(hype_list).union(set(hype_syns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c5eb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    dat_df[i] = 0\n",
    "    for ind, row in dat_df.iterrows():\n",
    "        # randomly replace hype words\n",
    "        rand_new_text = [t if t not in hype_syns else random.sample(hype_and_syns, 1)\n",
    "                            for t in row[\"tokens\"]]\n",
    "        # calculate new distribution\n",
    "        dat_df.loc[ind, i] = sum([1 if t in hype_list # or hype_and_syns?\n",
    "                                      else 0\n",
    "                                      for t in rand_new_text])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
